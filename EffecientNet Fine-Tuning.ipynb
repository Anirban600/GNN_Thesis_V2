{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abcd2945-4833-4fe6-8f0e-ea8c96bc10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import warnings\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863b4ea-b43e-444a-bb46-be42704e8e1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocess and save birdsnap and Food-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe0e5a9-366e-4a53-965d-fc49ed158ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0616c2a-e7c5-49ad-ab15-1b62ed8da127",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_dataset = datasets.ImageFolder(\"/media/extra_storage/anirban/birdsnap/download/images\", normalize)\n",
    "dataloader = DataLoader(bird_dataset, batch_size=128, shuffle=True, drop_last=True,\n",
    "                        collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a039451-5aa8-402d-aa84-c3b1eb6672de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for d in tqdm(bird_dataset): data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9cbc8-8491-46a4-8f5f-68ab753dbb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/extra_storage/anirban/birdsnap/bird_resized.pickle\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa3c7f-d628-457a-915f-db7b8b84b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/extra_storage/anirban/birdsnap/bird_resized.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f0c43-73fa-4124-8f3e-d998e2a13ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2a0b6ba-26d9-4bd5-8dc1-408420993960",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Lets do birdsnap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f064321-3888-4ea1-a6f1-3510d1276f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/anirban/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "effi_mod = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "\n",
    "\n",
    "effi_mod.classifier = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d(output_size=1),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(in_features=1280, out_features=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae71ac5-0efa-41bf-8a55-df70f8926de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2782067ea1de448d9dcda0e044a2aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 | Train Acc: 0.0027"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b2f500588443e4bd4d2ee4d9e12cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | Train Acc: 0.0025"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a87a7ca03c4dd8b13ae634c519a8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   2 | Train Acc: 0.0018"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8c1ebc99c742bbbce3cecabbc1b2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m     21\u001b[0m pred, actual \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     24\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/torchvision/datasets/folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/PIL/Image.py:901\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    858\u001b[0m ):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    903\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/PIL/ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         )\n\u001b[1;32m    256\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 257\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create model\n",
    "all_train, all_val, all_test = [], [], []\n",
    "time_taken, epochs_taken = [], []\n",
    "\n",
    "start = time.time()\n",
    "model = effi_mod\n",
    "model.to(device)\n",
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr=0.05, weight_decay=10e-5)\n",
    "\n",
    "\n",
    "def accuracy(a, b):\n",
    "    a = torch.cat(a)\n",
    "    b = torch.cat(b)\n",
    "    return (a == b).sum() / len(a)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    pred, actual = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        features = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, prediction = logits.max(axis=1)\n",
    "        pred.append(prediction)\n",
    "        actual.append(labels)\n",
    "        \n",
    "\n",
    "    train_acc = accuracy(pred, actual)\n",
    "    # val_acc = evaluate(model, features, labels.cuda(), val_mask.cuda())\n",
    "    # if args.early_stop:\n",
    "    #     if stopper.step(val_acc, model, epoch, args.print_less): break\n",
    "\n",
    "    print(f\"Epoch: {epoch:3d} | Train Acc: {train_acc:.4f}\", end=\"\")\n",
    "\n",
    "# if not args.print_less : print()\n",
    "# if args.early_stop: model.load_state_dict(torch.load('es_checkpoint.pt'))\n",
    "# train_acc = evaluate(model, features, labels.cuda(), train_mask.cuda())\n",
    "# val_acc = evaluate(model, features, labels.cuda(), val_mask.cuda())\n",
    "# test_acc = evaluate(model, features, labels.cuda(), test_mask.cuda())\n",
    "# best_epoch = epoch - 100\n",
    "\n",
    "# all_train.append(round(train_acc, 4))\n",
    "# all_val.append(round(val_acc, 4))\n",
    "# all_test.append(round(test_acc, 4))\n",
    "# time_taken.append(round(time.time() - start, 4))\n",
    "# epochs_taken.append(best_epoch)\n",
    "\n",
    "# if not args.print_less : print('-' * 100)\n",
    "# print(f\"Iteration: {i+1}/{args.n_iter} | Train Accuracy: {train_acc:.4f} | Val Accuracy: {val_acc:.4f} | Test Accuracy: {test_acc:.4f} | Time Taken: {time_taken[-1]}\")\n",
    "# if not args.print_less : print('-' * 100)\n",
    "\n",
    "\n",
    "# all_train, all_val, all_test, time_taken, epochs_taken = np.array(all_train), np.array(all_val), np.array(all_test), np.array(time_taken), np.array(epochs_taken)\n",
    "\n",
    "# print()\n",
    "# print(\"=\" * 100)\n",
    "# print(f\"Train Accuracy :     \\t{all_train}\")\n",
    "# print(f\"Validation Accuracy :\\t{all_val}\")\n",
    "# print(f\"Test Accuracy :      \\t{all_test}\")\n",
    "# print(f\"Time Taken :         \\t{time_taken}\")\n",
    "# print(f\"Epochs Taken :       \\t{epochs_taken}\")\n",
    "# print(f\"Average Train Accuracy :     \\t{all_train.mean():.4f} ± {all_train.std():.3f}\")\n",
    "# print(f\"Average Validation Accuracy :\\t{all_val.mean():.4f} ± {all_val.std():.3f}\")\n",
    "# print(f\"Average Test Accuracy :      \\t{all_test.mean():.4f} ± {all_test.std():.3f}\")\n",
    "# print(f\"Average Time Taken :         \\t{time_taken.mean():.4f}\")\n",
    "# print(f\"Average Epochs Taken :       \\t{epochs_taken.mean():.0f}\")\n",
    "# print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87d6e9-f0f1-43c9-ac04-c58967db3e7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split birdsnap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d646c08-ffe2-478e-914e-ef1cb27ee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "609ff4dc-a54b-4754-966e-20ba08317f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/extra_storage/anirban/birdsnap/test_images.txt\", 'r') as f:\n",
    "    tests = list(map(str.strip, f.readlines()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65a7ff74-4d46-4f33-94f5-fd1db450bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_root = \"/media/extra_storage/anirban/birdsnap/splits/train/\"\n",
    "val_root = \"/media/extra_storage/anirban/birdsnap/splits/val/\"\n",
    "test_root = \"/media/extra_storage/anirban/birdsnap/splits/test/\"\n",
    "\n",
    "for folder in os.listdir(src_root):\n",
    "    os.system(f\"mkdir {val_root}/{folder}\")\n",
    "    os.system(f\"mkdir {test_root}/{folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c53cb6eb-ccdf-420c-8df0-31f1fabe7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_root = \"/media/extra_storage/anirban/birdsnap/splits/train/\"\n",
    "dst_root = \"/media/extra_storage/anirban/birdsnap/splits/test/\"\n",
    "\n",
    "for image in tests:\n",
    "    src = os.path.join(src_root, image)\n",
    "    if os.path.exists(src):\n",
    "        dst = os.path.join(dst_root, image)\n",
    "        os.system(f\"mv {src} {dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e2381dc-b1f8-4ac1-a49e-d7bddedad104",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_root = \"/media/extra_storage/anirban/birdsnap/splits/train/\"\n",
    "dst_root = \"/media/extra_storage/anirban/birdsnap/splits/val/\"\n",
    "\n",
    "for folder in os.listdir(src_root):\n",
    "    folder_path = os.path.join(src_root, folder)\n",
    "    files = os.listdir(folder_path)\n",
    "    sample = random.sample(files, 3)\n",
    "    dst_path = os.path.join(dst_root, folder)\n",
    "    for file in sample:\n",
    "        src_path = os.path.join(folder_path, file)\n",
    "        os.system(f\"mv {src_path} {dst_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a152df2d-463e-403e-b05e-b683af760f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makeup less test image classes\n",
    "\n",
    "src_root = \"/media/extra_storage/anirban/birdsnap/splits/train/\"\n",
    "dst_root = \"/media/extra_storage/anirban/birdsnap/splits/test/\"\n",
    "\n",
    "for folder in os.listdir(dst_root):\n",
    "    count = len(os.listdir(os.path.join(dst_root, folder)))\n",
    "    if count < 3:\n",
    "        n = 3 - count\n",
    "        src_path = os.path.join(src_root, folder)\n",
    "        dst_path = os.path.join(dst_root, folder)\n",
    "        files = os.listdir(src_path)\n",
    "        sample = random.sample(files, n)\n",
    "        for file in sample:\n",
    "            src = os.path.join(src_path, file)\n",
    "            os.system(f\"mv {src} {dst_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14ba6797-3213-4bd6-aa1b-2d4216945f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 182, 4: 175, 5: 143}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst_root = \"/media/extra_storage/anirban/birdsnap/splits/test/\"\n",
    "\n",
    "d = dict()\n",
    "for folder in os.listdir(dst_root):\n",
    "    count = len(os.listdir(os.path.join(dst_root, folder)))\n",
    "    if count in d: d[count] += 1\n",
    "    else: d[count] = 1\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5098d-33a8-4a19-9884-464132fadd46",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e59a9a-b5c3-49e6-92b6-fedad7fda122",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3733f03-d06e-46da-b7ca-2320986aeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bird_train_data = datasets.ImageFolder(\"/media/extra_storage/anirban/birdsnap/splits/train/\", normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8fb035-6132-4295-ab74-fde788e968fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54db7d67d2442c495f6afb52141abf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cur_label = 0\n",
    "data = []\n",
    "\n",
    "for bird, label in tqdm(bird_train_data):\n",
    "    if label != cur_label:\n",
    "        with open(f\"/media/extra_storage/anirban/birdsnap/dataloaders/class_{label}.pickle\", 'wb') as f: pickle.dump(data, f)\n",
    "        data = []\n",
    "        cur_label = label\n",
    "    data.append((bird, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49bfd796-78a6-4583-bcbf-305160336933",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/media/extra_storage/anirban/birdsnap/dataloaders/class_500.pickle\", 'wb') as f: pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f6bd0f-58f1-454d-aa38-b30714772bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        self._data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ea03fc-9878-426e-8240-96b40deb27e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099f454609ac4320afe03081c0c7142c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "root = \"/media/extra_storage/anirban/birdsnap/dataloaders/\"\n",
    "\n",
    "dataset_list = []\n",
    "for file in tqdm(os.listdir(root)):\n",
    "    file_path = os.path.join(root, file)\n",
    "    dataset_list.append(MyDataset(file_path))\n",
    "\n",
    "print(len(dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d82dd0-0c05-480e-adb0-e961ab3a68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_data = ConcatDataset(dataset_list)\n",
    "dataloader = DataLoader(concat_data, batch_size=128, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d286b1-541b-490f-afa6-0ca00ce54b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Validation and test sets\n",
    "\n",
    "bird_test_data = datasets.ImageFolder(\"/media/extra_storage/anirban/birdsnap/splits/test/\", normalize)\n",
    "bird_val_data = datasets.ImageFolder(\"/media/extra_storage/anirban/birdsnap/splits/val/\", normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f6286a-0e30-40ca-a6b0-ded774d597eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d275324ab34a47af8da2c54310de3b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7284b78734e34679a62dba49259a5cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for datapoint in tqdm(bird_test_data): data.append(datapoint)\n",
    "with open(f\"/media/extra_storage/anirban/birdsnap/dataloaders/test.pickle\", 'wb') as f: pickle.dump(data, f)\n",
    "\n",
    "data = []\n",
    "\n",
    "for datapoint in tqdm(bird_val_data): data.append(datapoint)\n",
    "with open(f\"/media/extra_storage/anirban/birdsnap/dataloaders/val.pickle\", 'wb') as f: pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3dbf3e-ea4c-4e36-9bb5-92ef3ed4bec6",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
